{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syn-motifs Generation Pipeline\n",
    "\n",
    "This notebook implements an open-source version of the Syn-motifs generation pipeline\n",
    "used to design Synmask, an intrinsically disordered, hydrophilic and low-immunogenicity\n",
    "polypeptide used to extend protein half-life without perturbing its intrinsic function.\n",
    "\n",
    "The pipeline consists of 5 main steps:\n",
    "\n",
    "1. Screening disordered fragments from natural proteins\n",
    "2. Removing fragments containing unreasonable motifs\n",
    "3. Removing fragments containing unreasonable amino acids\n",
    "4. Removing fragments predicted to contain secondary structures\n",
    "5. Amino acid modification and re-prediction\n",
    "\n",
    "Each section of this notebook corresponds to one step in the Methods and can be run\n",
    "independently, assuming the corresponding input files from the previous step are available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 Screening disordered fragments from natural proteins\n",
    "## Step 1.1 Find all disordered fragments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function used in step1.1\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "import pathlib\n",
    "\n",
    "def generate_dssp_shell(pdb_dir, dssp_out_dir, shell_path=\"run_dssp.sh\"):\n",
    "    \"\"\"\n",
    "    Generate a shell script that runs DSSP for each PDB/mmCIF file in `pdb_dir`.\n",
    "\n",
    "    DSSP command example:\n",
    "        dssp input.pdb output.dssp\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pdb_dir : str\n",
    "        Path to directory containing PDB files.\n",
    "    dssp_out_dir : str\n",
    "        Directory to store DSSP output files.\n",
    "    shell_path : str\n",
    "        Output shell script path.\n",
    "    \"\"\"\n",
    "    os.makedirs(dssp_out_dir, exist_ok=True)\n",
    "\n",
    "    with open(shell_path, \"w\") as f:\n",
    "        for file in sorted(os.listdir(pdb_dir)):\n",
    "            if file.endswith(\".pdb\") or file.endswith(\".cif\"):\n",
    "                in_path = os.path.join(pdb_dir, file)\n",
    "                out_name = pathlib.Path(file).stem + \".dssp\"\n",
    "                out_path = os.path.join(dssp_out_dir, out_name)\n",
    "                f.write(f\"dssp {in_path} {out_path}\\n\")\n",
    "\n",
    "    print(f\"[OK] DSSP shell written to: {shell_path}\")\n",
    "\n",
    "def parse_dssp_file(dssp_path):\n",
    "    \"\"\"\n",
    "    Parse a DSSP file and extract (chain, resnum, aa, ss) information.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict: mapping\n",
    "        { chain: [ (resnum, aa, ss), ... ] }\n",
    "    \"\"\"\n",
    "    chain_map = defaultdict(list)\n",
    "    header_passed = False\n",
    "\n",
    "    with open(dssp_path) as f:\n",
    "        for line in f:\n",
    "            \n",
    "            if line.strip().startswith(\"#\"):\n",
    "                header_passed = True\n",
    "                continue\n",
    "\n",
    "            if not header_passed:\n",
    "                continue\n",
    "\n",
    "            parts = line.rstrip(\"\\n\")\n",
    "            if len(parts) < 120:\n",
    "                continue\n",
    "\n",
    "            resnum_str = parts[5:10].strip()\n",
    "            if not resnum_str.isdigit():\n",
    "                continue\n",
    "            resnum = int(resnum_str)\n",
    "            chain = parts[11].strip() or \"_\"\n",
    "            aa = parts[13]\n",
    "            ss = parts[16] if parts[16] != \" \" else \"-\"\n",
    "            \n",
    "            chain_map[chain].append((resnum, aa, ss))\n",
    "    return chain_map\n",
    "\n",
    "def extract_disordered_fragments(chain_res_list, min_len=20):\n",
    "    \"\"\"\n",
    "    Extract continuous disordered fragments from one chain.\n",
    "\n",
    "    A residue is considered disordered if:\n",
    "        ss == '-'\n",
    "\n",
    "    Breaking conditions:\n",
    "        - residue index not consecutive\n",
    "        - secondary structure not '-'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of dicts:\n",
    "        { \"start\": int, \"end\": int, \"sequence\": str }\n",
    "    \"\"\"\n",
    "    frags = []\n",
    "    cur_seq = []\n",
    "    cur_start = None\n",
    "    prev_resnum = None\n",
    "\n",
    "    for resnum, aa, ss in chain_res_list:\n",
    "        is_disordered = (ss == \"-\" or ss == 'T' or ss == 'S')\n",
    "\n",
    "        if is_disordered:\n",
    "            # Start a new fragment if needed\n",
    "            if cur_start is None or prev_resnum is None or resnum != prev_resnum + 1:\n",
    "                # flush previous fragment\n",
    "                if cur_seq and len(cur_seq) >= min_len:\n",
    "                    frags.append({\n",
    "                        \"start\": cur_start,\n",
    "                        \"end\": prev_resnum,\n",
    "                        \"sequence\": \"\".join(cur_seq)\n",
    "                    })\n",
    "                # start new fragment\n",
    "                cur_start = resnum\n",
    "                cur_seq = []\n",
    "\n",
    "            cur_seq.append(aa)\n",
    "        else:\n",
    "            # structured → flush fragment\n",
    "            if cur_seq and len(cur_seq) >= min_len:\n",
    "                frags.append({\n",
    "                    \"start\": cur_start,\n",
    "                    \"end\": prev_resnum,\n",
    "                    \"sequence\": \"\".join(cur_seq)\n",
    "                })\n",
    "            cur_seq = []\n",
    "            cur_start = None\n",
    "\n",
    "        prev_resnum = resnum\n",
    "\n",
    "    # flush last fragment\n",
    "    if cur_seq and len(cur_seq) >= min_len:\n",
    "        frags.append({\n",
    "            \"start\": cur_start,\n",
    "            \"end\": prev_resnum,\n",
    "            \"sequence\": \"\".join(cur_seq)\n",
    "        })\n",
    "\n",
    "    return frags\n",
    "\n",
    "def extract_all_disordered_fragments(dssp_dir, output_csv, min_len=20):\n",
    "    \"\"\"\n",
    "    Process all DSSP files in a directory and extract\n",
    "    contiguous disordered fragments.\n",
    "\n",
    "    Output CSV columns:\n",
    "        pdb_id, chain, start, end, sequence\n",
    "    \"\"\"\n",
    "    files = sorted([f for f in os.listdir(dssp_dir) if f.endswith(\".dssp\")])\n",
    "\n",
    "    with open(output_csv, \"w\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"pdb_id\", \"chain\", \"start\", \"end\", \"sequence\"])\n",
    "\n",
    "        for file in tqdm(files):\n",
    "            pdb_id = pathlib.Path(file).stem\n",
    "            dssp_path = os.path.join(dssp_dir, file)\n",
    "            chain_map = parse_dssp_file(dssp_path)\n",
    "\n",
    "            for chain, residues in chain_map.items():\n",
    "                frags = extract_disordered_fragments(residues, min_len=min_len)\n",
    "                for frag in frags:\n",
    "                    writer.writerow([\n",
    "                        pdb_id,\n",
    "                        chain,\n",
    "                        frag[\"start\"],\n",
    "                        frag[\"end\"],\n",
    "                        frag[\"sequence\"],\n",
    "                    ])\n",
    "\n",
    "    print(f\"[OK] Step 1 result written to {output_csv}\")\n",
    "\n",
    "def deduplicate_fragments(input_csv, output_csv, min_len=20):\n",
    "    \"\"\"\n",
    "    Remove duplicate sequences and short fragments.\n",
    "\n",
    "    Only keep rows with sequence length >= min_len.\n",
    "    \"\"\"\n",
    "    seen = set()\n",
    "    out = []\n",
    "\n",
    "    with open(input_csv) as f:\n",
    "        next(f)  # skip header\n",
    "        for line in f:\n",
    "            pdb_id, chain, start, end, seq = line.strip().split(\",\")\n",
    "            \n",
    "            if len(seq) < min_len:\n",
    "                continue\n",
    "            if \"X\" in seq:\n",
    "                continue\n",
    "            if seq in seen:\n",
    "                continue\n",
    "            seen.add(seq)\n",
    "            out.append([f\"{pdb_id}_{chain}\", seq, len(seq)])\n",
    "\n",
    "    with open(output_csv, \"w\") as wf:\n",
    "        writer = csv.writer(wf)\n",
    "        writer.writerow([\"PDB_id\", \"Sequence\", \"Length\"])\n",
    "        writer.writerows(out)\n",
    "\n",
    "    print(f\"[OK] Deduplicated fragments written to {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_dir = '/mnt/data/public/PDB_split'\n",
    "dssp_output = './dssp_results/'\n",
    "bash_file = './run_dssp.sh'\n",
    "generate_dssp_shell(pdb_dir, dssp_output, bash_file)\n",
    "\n",
    "# bash ./run_dssp.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = './all_disordered.csv'\n",
    "extract_all_disordered_fragments(dssp_output, output_file)\n",
    "\n",
    "du_output_file = './step1_disordered.csv'\n",
    "deduplicate_fragments(output_file, du_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.2 Find all missing fragments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from Bio.PDB import PDBParser\n",
    "from Bio import pairwise2\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# === 3-letter → 1-letter mapping ===\n",
    "three_to_one = {\n",
    "    'ALA':'A','ARG':'R','ASN':'N','ASP':'D','CYS':'C','GLU':'E','GLN':'Q',\n",
    "    'GLY':'G','HIS':'H','ILE':'I','LEU':'L','LYS':'K','MET':'M','PHE':'F',\n",
    "    'PRO':'P','SER':'S','THR':'T','TRP':'W','TYR':'Y','VAL':'V',\n",
    "    'MSE':'M'  # selenium-methionine\n",
    "}\n",
    "\n",
    "VALID_AA_3 = set(three_to_one.keys())\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "#                 Load SEQRES FASTA\n",
    "# ==========================================================\n",
    "def load_seqres(seqres_path):\n",
    "    \"\"\"\n",
    "    Load SEQRES FASTA.  \n",
    "    Return dict: { '3nch_C': 'SEQUENCE', ... }\n",
    "    \"\"\"\n",
    "    seqres = {}\n",
    "    name = None\n",
    "    with open(seqres_path) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\">\"):\n",
    "                name = line.split()[0][1:]\n",
    "                seqres[name] = \"\"\n",
    "            else:\n",
    "                seqres[name] += line\n",
    "    return seqres\n",
    "\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "#          Extract protein sequence from ATOM records\n",
    "# ==========================================================\n",
    "def get_observed_sequence(pdb_file):\n",
    "    \"\"\"\n",
    "    Return a sequence string extracted from ATOM coordinates, **protein only**.\n",
    "    Residues are in the order they appear in the chain.\n",
    "    \"\"\"\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure(\"pdb\", pdb_file)\n",
    "    seq = []\n",
    "\n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            for res in chain.get_residues():\n",
    "                hetflag, resseq, icode = res.id\n",
    "                resname = res.resname.upper()\n",
    "\n",
    "                # only standard amino acids (exclude DNA/RNA)\n",
    "                if hetflag == \" \" and resname in VALID_AA_3 and \"CA\" in res.child_dict:\n",
    "                    seq.append(three_to_one[resname])\n",
    "            break\n",
    "        break\n",
    "    return \"\".join(seq)\n",
    "\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "#            Align SEQRES ↔ ATOM sequence\n",
    "# ==========================================================\n",
    "def align_sequences(seq_full, seq_observed):\n",
    "    \"\"\"\n",
    "    Global alignment to map SEQRES ↔ ATOM.\n",
    "    Return: aligned_seqres, aligned_observed\n",
    "    \"\"\"\n",
    "    align = pairwise2.align.globalms(seq_full, seq_observed,\n",
    "                                     2, -1, -10, -1,\n",
    "                                     one_alignment_only=True)[0]\n",
    "    return align.seqA, align.seqB\n",
    "\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "#            Identify missing internal segments\n",
    "# ==========================================================\n",
    "def find_missing_segments_from_alignment(aln_seqres, aln_observed, pdbid_chain):\n",
    "    \"\"\"\n",
    "    aln_seqres : aligned SEQRES\n",
    "    aln_observed : aligned ATOM seq\n",
    "    A missing residue = position where:\n",
    "        aln_seqres[i] != '-' AND aln_observed[i] == '-'\n",
    "    \"\"\"\n",
    "\n",
    "    missing_segments = []\n",
    "    n = len(aln_seqres)\n",
    "\n",
    "    cur = []\n",
    "    for i in range(n):\n",
    "        aa_seqres = aln_seqres[i]\n",
    "        aa_obs = aln_observed[i]\n",
    "\n",
    "        if aa_seqres != '-' and aa_obs == '-':\n",
    "            cur.append(aa_seqres)\n",
    "        else:\n",
    "            if cur:\n",
    "                missing_segments.append(\"\".join(cur))\n",
    "                cur = []\n",
    "    if cur:\n",
    "        missing_segments.append(\"\".join(cur))\n",
    "\n",
    "    # Generate final entries with metadata\n",
    "    results = []\n",
    "    seqres_pos = 0  # SEQRES index (1-based)\n",
    "    obs_pos = 0     # ATOM index (1-based, not used directly)\n",
    "\n",
    "    cur_start = None\n",
    "    cur_end = None\n",
    "    cur_seq = []\n",
    "\n",
    "    for i in range(n):\n",
    "        aa_seqres = aln_seqres[i]\n",
    "        aa_obs = aln_observed[i]\n",
    "\n",
    "        if aa_seqres != \"-\":\n",
    "            seqres_pos += 1\n",
    "\n",
    "        # missing residue\n",
    "        if aa_seqres != \"-\" and aa_obs == \"-\":\n",
    "            if cur_start is None:\n",
    "                cur_start = seqres_pos\n",
    "            cur_end = seqres_pos\n",
    "            cur_seq.append(aa_seqres)\n",
    "        else:\n",
    "            if cur_seq:\n",
    "                results.append(\n",
    "                    (cur_start, cur_end, \"\".join(cur_seq))\n",
    "                )\n",
    "                cur_start = None\n",
    "                cur_end = None\n",
    "                cur_seq = []\n",
    "\n",
    "    if cur_seq:\n",
    "        results.append(\n",
    "            (cur_start, cur_end, \"\".join(cur_seq))\n",
    "        )\n",
    "\n",
    "    # Build output dicts\n",
    "    final = []\n",
    "    for (s, e, seq) in results:\n",
    "        at_head = (s == 1)\n",
    "        at_tail = (e == aln_seqres.replace(\"-\", \"\").__len__())\n",
    "        at_headtail = \"yes\" if (at_head or at_tail) else \"no\"\n",
    "\n",
    "        final.append({\n",
    "            \"pdbid\": pdbid_chain,\n",
    "            \"seqres_start\": s,\n",
    "            \"seqres_end\": e,\n",
    "            \"pdb_start\": None,   # no need anymore\n",
    "            \"pdb_end\": None,\n",
    "            \"missing_seq\": seq,\n",
    "            \"seq_len\": len(seq),\n",
    "            \"at_headtail\": at_headtail\n",
    "        })\n",
    "    return final\n",
    "\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "#                   Step 1.2 main function\n",
    "# ==========================================================\n",
    "def extract_missing_regions(pdb_dir, seqres_path, output_csv):\n",
    "    seqres_map = load_seqres(seqres_path)\n",
    "\n",
    "    with open(output_csv, \"w\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\n",
    "            \"pdbid\", \"seqres_start\", \"seqres_end\",\n",
    "            \"pdb_start\", \"pdb_end\",\n",
    "            \"missing_seq\", \"seq_len\", \"at_headtail\"\n",
    "        ])\n",
    "\n",
    "        for file in tqdm(sorted(os.listdir(pdb_dir))):\n",
    "            if not file.endswith(\".pdb\"):\n",
    "                continue\n",
    "\n",
    "            pdbid_chain = file.replace(\".pdb\", \"\")\n",
    "            if pdbid_chain not in seqres_map:\n",
    "                continue\n",
    "\n",
    "            seq_full = seqres_map[pdbid_chain]\n",
    "            seq_obs = get_observed_sequence(os.path.join(pdb_dir, file))\n",
    "\n",
    "            if len(seq_obs) == 0:\n",
    "                continue  # skip empty chains\n",
    "\n",
    "            alnA, alnB = align_sequences(seq_full, seq_obs)\n",
    "\n",
    "            segs = find_missing_segments_from_alignment(alnA, alnB, pdbid_chain)\n",
    "\n",
    "            for seg in segs:\n",
    "                writer.writerow([\n",
    "                    seg[\"pdbid\"],\n",
    "                    seg[\"seqres_start\"],\n",
    "                    seg[\"seqres_end\"],\n",
    "                    seg[\"pdb_start\"],\n",
    "                    seg[\"pdb_end\"],\n",
    "                    seg[\"missing_seq\"],\n",
    "                    seg[\"seq_len\"],\n",
    "                    seg[\"at_headtail\"],\n",
    "                ])\n",
    "\n",
    "    print(f\"[OK] Missing regions written to {output_csv}\")\n",
    "\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "#             Deduplication & fragment filtering\n",
    "# ==========================================================\n",
    "def deduplicate_missing(input_csv, output_csv, min_len=20):\n",
    "\n",
    "    seen = set()\n",
    "\n",
    "    with open(output_csv, \"w\") as wf:\n",
    "        writer = csv.writer(wf)\n",
    "        writer.writerow([\"PDB_id\", \"Sequence\", \"Length\"])\n",
    "\n",
    "        with open(input_csv) as f:\n",
    "            header = next(f).strip().split(\",\")\n",
    "            col = {h: i for i, h in enumerate(header)}\n",
    "\n",
    "            for line in f:\n",
    "                parts = line.strip().split(\",\")\n",
    "\n",
    "                pdbid = parts[col[\"pdbid\"]]\n",
    "                seq = parts[col[\"missing_seq\"]]\n",
    "                length = int(parts[col[\"seq_len\"]])\n",
    "                at_headtail = parts[col[\"at_headtail\"]]\n",
    "\n",
    "                # filters\n",
    "                if at_headtail != \"no\":     # remove N/C terminal missing\n",
    "                    continue\n",
    "                if length < min_len:\n",
    "                    continue\n",
    "                if \"X\" in seq:\n",
    "                    continue\n",
    "                if seq in seen:\n",
    "                    continue\n",
    "\n",
    "                seen.add(seq)\n",
    "\n",
    "                writer.writerow([pdbid, seq, length])\n",
    "\n",
    "    print(f\"[OK] Deduplicated file written to {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_dir = '/mnt/data/public/split_PDB'\n",
    "seq_res_file = '/mnt/data/public/pdb_seqres.txt'\n",
    "output_file = 'all_missing.csv'\n",
    "extract_missing_regions(pdb_dir, seq_res_file, output_file)\n",
    "du_output_file = \"./step1_missing.csv\"\n",
    "deduplicate_missing(output_file,du_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step2 Removing fragments containing unreasonable motifs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 1. Homopolymer ≥ 4\n",
    "# -------------------------\n",
    "def has_homopolymer(seq, run_len=4):\n",
    "    \"\"\"\n",
    "    Return True if the sequence contains a homopolymeric run\n",
    "    of length >= run_len (e.g., 'SSSS' for run_len=4).\n",
    "    \"\"\"\n",
    "    s = seq.upper()\n",
    "    if len(s) < run_len:\n",
    "        return False\n",
    "\n",
    "    count = 1\n",
    "    for i in range(1, len(s)):\n",
    "        if s[i] == s[i - 1]:\n",
    "            count += 1\n",
    "            if count >= run_len:\n",
    "                return True\n",
    "        else:\n",
    "            count = 1\n",
    "    return False\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 2. Linker motifs:\n",
    "#    - explicit 'GGGGS'\n",
    "#    - any 'GGGX' (GGG + any residue)\n",
    "# -------------------------\n",
    "def contains_linker_motif(seq):\n",
    "    \"\"\"\n",
    "    Return True if the sequence contains linker-like patterns that\n",
    "    we want to exclude, including:\n",
    "      - 'GGGGS'\n",
    "      - any motif 'GGGX' (GGG followed by any residue).\n",
    "    \"\"\"\n",
    "    s = seq.upper()\n",
    "\n",
    "    if \"GGGGS\" in s:\n",
    "        return True\n",
    "\n",
    "    # GGG + any amino acid\n",
    "    if re.search(r\"GGG.\", s):\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 3. Non-canonical amino acids\n",
    "# -------------------------\n",
    "def contains_noncanonical(seq):\n",
    "    \"\"\"\n",
    "    Return True if the sequence contains residues outside the\n",
    "    20 standard amino acids (ACDEFGHIKLMNPQRSTVWY).\n",
    "    'X' is automatically treated as non-canonical.\n",
    "    \"\"\"\n",
    "    valid = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "    s = seq.upper()\n",
    "    return any(aa not in valid for aa in s)\n",
    "\n",
    "\n",
    "\n",
    "def contains_short_repeats(seq, min_len=3, max_len=6, min_repeats=3):\n",
    "    \"\"\"\n",
    "    Return True if the sequence contains short repetitive motifs:\n",
    "      - motif length in [min_len, max_len]\n",
    "      - repeated >= min_repeats times (overlapping allowed)\n",
    "\n",
    "    Example: 'GSSGSSGSS' will be flagged because 'GSS' appears >= 3 times.\n",
    "    \"\"\"\n",
    "    s = seq.upper()\n",
    "    n = len(s)\n",
    "    if n < min_len:\n",
    "        return False\n",
    "\n",
    "    for k in range(min_len, max_len + 1):\n",
    "        freq = {}\n",
    "        for i in range(0, n - k + 1):\n",
    "            mot = s[i:i + k]\n",
    "            freq[mot] = freq.get(mot, 0) + 1\n",
    "\n",
    "        if any(count >= min_repeats for count in freq.values()):\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Step 2 main function\n",
    "# -------------------------\n",
    "\n",
    "def is_valid_protein_sequence(seq):\n",
    "    \"\"\"\n",
    "    Ensure the sequence contains ONLY the 20 canonical amino acids.\n",
    "    \"\"\"\n",
    "    valid = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "    return all(aa in valid for aa in seq.upper())\n",
    "\n",
    "\n",
    "def step2_filter_unreasonable_motifs(input_csv, output_csv):\n",
    "    kept_rows = []\n",
    "\n",
    "    with open(input_csv) as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            seq = row[\"Sequence\"].strip().upper()\n",
    "\n",
    "            # PRE-FILTER: remove nucleic-acid sequences\n",
    "            if not is_valid_protein_sequence(seq):\n",
    "                continue\n",
    "\n",
    "            # Rule 1: homopolymer runs\n",
    "            if has_homopolymer(seq):\n",
    "                continue\n",
    "\n",
    "            # Rule 2: linker-like GGGX patterns\n",
    "            if contains_linker_motif(seq):\n",
    "                continue\n",
    "\n",
    "            # Rule 3: non-canonical AA\n",
    "            if contains_noncanonical(seq):\n",
    "                continue\n",
    "\n",
    "            # Rule 4: short repeat motifs (3–6 aa repeated >=3)\n",
    "            if contains_short_repeats(seq):\n",
    "                continue\n",
    "\n",
    "            kept_rows.append(row)\n",
    "\n",
    "    with open(output_csv, \"w\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"PDB_id\", \"Sequence\", \"Length\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(kept_rows)\n",
    "\n",
    "    print(f\"[OK] Step2 complete: kept {len(kept_rows)} → {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step1_missing_file = './step1_missing.csv'\n",
    "step1_disordered_file = './step1_disordered.csv'\n",
    "\n",
    "step2_missing_file = './step2_missing.csv'\n",
    "step2_disordered_file = './step2_disordered.csv'\n",
    "\n",
    "step2_filter_unreasonable_motifs(input_csv=step1_disordered_file, output_csv=step2_disordered_file)\n",
    "step2_filter_unreasonable_motifs(input_csv=step1_missing_file, output_csv=step2_missing_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 Removing fragments containing unreasonable amino acids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ============================================================\n",
    "# Amino-acid class definitions\n",
    "# ============================================================\n",
    "\n",
    "# AA7: preferred hydrophilic & flexible amino acids\n",
    "AA7 = set(list(\"AGTSPED\"))\n",
    "\n",
    "# AA4: tolerated but should not dominate the sequence\n",
    "AA4 = set(list(\"HLVY\"))\n",
    "\n",
    "# AA9: undesirable residues (hydrophobic, unstable, or positively charged)\n",
    "AA9 = set(list(\"RNCQIKMFW\"))\n",
    "\n",
    "# Full set of allowed amino acids\n",
    "VALID_AA = AA7 | AA4 | AA9\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Step 3: Filtering based on amino-acid composition\n",
    "# ============================================================\n",
    "\n",
    "def step3_filter_amino_acids(\n",
    "    input_csv,\n",
    "    output_csv,\n",
    "    aa7_min_ratio=0.70,       # Minimum required ratio for AA7 residues\n",
    "    aa9_max_ratio=0.10,       # Maximum allowed ratio for AA9 residues\n",
    "    agtspe_max_ratio=0.85     # Maximum allowed ratio for AGTSPE residues\n",
    "):\n",
    "    \"\"\"\n",
    "    Filter peptide fragments based on amino-acid class composition.\n",
    "\n",
    "    Conditions:\n",
    "        - AA7 (AGTSPED) ≥ 70%\n",
    "        - AA9 (RNCQIKMFW) ≤ 10%\n",
    "        - AGTSPE ≤ 85% (to avoid overly homogeneous composition)\n",
    "\n",
    "    Input/Output format:\n",
    "        CSV with columns: PDB_id, Sequence, Length\n",
    "    \"\"\"\n",
    "    kept_rows = []\n",
    "\n",
    "    with open(input_csv) as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in tqdm(reader):\n",
    "            seq = row[\"Sequence\"].strip().upper()\n",
    "            length = len(seq)\n",
    "\n",
    "            # Skip sequences containing non-standard residues (should rarely happen)\n",
    "            if not all(aa in VALID_AA for aa in seq):\n",
    "                continue\n",
    "\n",
    "            # Count category occurrences\n",
    "            aa7_count = sum(aa in AA7 for aa in seq)\n",
    "            aa4_count = sum(aa in AA4 for aa in seq)\n",
    "            aa9_count = sum(aa in AA9 for aa in seq)\n",
    "\n",
    "            aa7_ratio = aa7_count / length\n",
    "            aa9_ratio = aa9_count / length\n",
    "\n",
    "            # Special filter: prevent AGTSPE from dominating excessively\n",
    "            agtspe_set = set(list(\"AGTSPE\"))\n",
    "            agtspe_ratio = sum(aa in agtspe_set for aa in seq) / length\n",
    "\n",
    "            # ---------------------------------------------------------\n",
    "            # Apply filtering rules\n",
    "            # ---------------------------------------------------------\n",
    "\n",
    "            # AA7 must be the dominant class (≥ 70%)\n",
    "            if aa7_ratio < aa7_min_ratio:\n",
    "                continue\n",
    "\n",
    "            # AA9 (undesirable residues) must remain low (≤ 10%)\n",
    "            if aa9_ratio > aa9_max_ratio:\n",
    "                continue\n",
    "\n",
    "            # Avoid overly homogeneous AGTSPE-only sequences (≤ 85%)\n",
    "            if agtspe_ratio > agtspe_max_ratio:\n",
    "                continue\n",
    "\n",
    "            # Sequence passes all filters → keep it\n",
    "            kept_rows.append(row)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Write filtered results\n",
    "    # ---------------------------------------------------------\n",
    "    with open(output_csv, \"w\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"PDB_id\", \"Sequence\", \"Length\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(kept_rows)\n",
    "\n",
    "    print(f\"[OK] Step 3 complete: kept {len(kept_rows)} fragments → {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step2_missing_file = './step2_missing.csv'\n",
    "step2_disordered_file = './step2_disordered.csv'\n",
    "\n",
    "step3_missing_file = './step3_missing.csv'\n",
    "step3_disordered_file = './step3_disordered.csv'\n",
    "\n",
    "step3_filter_amino_acids(input_csv=step2_missing_file, output_csv=step3_missing_file)\n",
    "step3_filter_amino_acids(input_csv=step2_disordered_file, output_csv=step3_disordered_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 Removing fragments predicted to contain secondary structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 — S4PRED: build merged FASTA\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def make_fasta_from_step3(step3_csv, out_fasta):\n",
    "    \"\"\"\n",
    "    Convert Step3 CSV -> FASTA file for S4PRED input.\n",
    "    CSV format: PDB_id, Sequence, Length\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(step3_csv)\n",
    "    seqs = df[\"Sequence\"].to_list()\n",
    "\n",
    "    with open(out_fasta, \"w\") as f:\n",
    "        for i, seq in enumerate(seqs):\n",
    "            f.write(f\">{i}\\n{seq}\\n\")\n",
    "\n",
    "    print(f\"[OK] FASTA written → {out_fasta}\")\n",
    "    return len(seqs)\n",
    "\n",
    "# Example usage:\n",
    "make_fasta_from_step3(\"step3_missing.csv\",\"step4_missing_s4pred_input.fasta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run S4PRED\n",
    "\n",
    "Use the repository: https://github.com/psipred/s4pred\n",
    "\n",
    "After installation, run the following command:\n",
    "\n",
    "python /home/wangyu/gitlab/ss_predict/s4pred-update/run_model.py --device gpu --outfmt fas ./step4_missing_s4pred_input.fasta > ./step4_missing.fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse S4PRED output: each 3 lines → (seq, ss, percent_C)\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def parse_s4pred(s4pred_fasta, out_csv):\n",
    "    seqs, ss_list, percents = [], [], []\n",
    "\n",
    "    with open(s4pred_fasta) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for i, line in enumerate(tqdm(lines)):\n",
    "        if i % 3 == 1:\n",
    "            seqs.append(line.strip())\n",
    "        if i % 3 == 2:\n",
    "            ss = line.strip()\n",
    "            ss_list.append(ss)\n",
    "            percents.append(ss.count(\"C\") / len(ss))\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"id\": range(len(seqs)),\n",
    "        \"Sequence\": seqs,\n",
    "        \"s4_ss\": ss_list,\n",
    "        \"s4_coil_percent\": percents,\n",
    "    })\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(f\"[OK] S4PRED parsed → {out_csv}\")\n",
    "\n",
    "# Filter: keep coils ≥ threshold\n",
    "import pandas as pd\n",
    "\n",
    "# Filter: keep coils ≥ threshold\n",
    "def filter_s4pred_by_coil(s4_csv, fasta_input, fasta_output, coil_th=0.99):\n",
    "    df = pd.read_csv(s4_csv)\n",
    "    keep_ids = df[df[\"s4_coil_percent\"] >= coil_th][\"id\"].to_list()\n",
    "\n",
    "    seq_lines = open(fasta_input).read().split()\n",
    "    seq_out = []\n",
    "\n",
    "    # FASTA is \">id\", \"SEQ\"\n",
    "    for i, line in enumerate(seq_lines):\n",
    "        if i % 2 == 0:  # header\n",
    "            idx = int(line[1:])\n",
    "            if idx in keep_ids:\n",
    "                seq_out.append(line)\n",
    "                seq_out.append(seq_lines[i+1])\n",
    "\n",
    "    with open(fasta_output, \"w\") as f:\n",
    "        f.write(\"\\n\".join(seq_out) + \"\\n\")\n",
    "\n",
    "    print(f\"[OK] S4PRED filter done → {fasta_output}\")\n",
    "\n",
    "parse_s4pred('step4_missing.fasta','step4_missing_S4PRED.csv')\n",
    "filter_s4pred_by_coil('step4_missing_S4PRED.csv','step4_missing.fasta','step4_missing_S4PRED_output.fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ProtBert secondary structure prediction\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def protbert_predict(fasta, out_csv):\n",
    "    # Read sequences\n",
    "    seqs = []\n",
    "    with open(fasta) as f:\n",
    "        tokens = f.read().split()\n",
    "        for line in tokens:\n",
    "            if not line.startswith(\">\"):\n",
    "                seqs.append(line)\n",
    "\n",
    "    # format for ProtBert (add space between AA)\n",
    "    spaced = [\" \".join(list(s)) for s in seqs]\n",
    "\n",
    "    pipeline = TokenClassificationPipeline(\n",
    "        model=AutoModelForTokenClassification.from_pretrained(\"Rostlab/prot_bert_bfd_ss3\"),\n",
    "        tokenizer=AutoTokenizer.from_pretrained(\"Rostlab/prot_bert_bfd_ss3\"),\n",
    "        device=0\n",
    "    )\n",
    "\n",
    "    # run model\n",
    "    seqs_clean = [re.sub(r\"[UZOB]\", \"X\", s) for s in spaced]\n",
    "    results = pipeline(seqs_clean)\n",
    "\n",
    "    # count coil ratios\n",
    "    coil_ratio = []\n",
    "    ss_strings = []\n",
    "\n",
    "    for res in results:\n",
    "        ss = \"\".join([x[\"entity\"] for x in res])\n",
    "        ss_strings.append(ss)\n",
    "        coil_ratio.append(ss.count(\"C\") / len(ss))\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"id\": range(len(seqs)),\n",
    "        \"Sequence\": seqs,\n",
    "        \"protbert_ss\": ss_strings,\n",
    "        \"protbert_coil_percent\": coil_ratio\n",
    "    })\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(f\"[OK] ProtBert results → {out_csv}\")\n",
    "\n",
    "protbert_predict('step4_missing_S4PRED_output.fasta','step4_missing_ProtBert.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_protbert(s4_filtered_fasta, protbert_csv, out_fasta, th=0.97):\n",
    "    df = pd.read_csv(protbert_csv)\n",
    "    keep_ids = df[df[\"protbert_coil_percent\"] >= th][\"id\"].to_list()\n",
    "\n",
    "    lines = open(s4_filtered_fasta).read().split()\n",
    "    out = []\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        if i % 2 == 0:\n",
    "            idx = int(line[1:])\n",
    "            if idx in keep_ids:\n",
    "                out.append(line)\n",
    "                out.append(lines[i+1])\n",
    "\n",
    "    with open(out_fasta, \"w\") as f:\n",
    "        f.write(\"\\n\".join(out) + \"\\n\")\n",
    "\n",
    "    print(f\"[OK] ProtBert filtered → {out_fasta}\")\n",
    "\n",
    "filter_protbert(\"step4_missing_S4PRED_output.fasta\",\"step4_missing_ProtBert.csv\",\"step4_missing_ProtBert_output.fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AF2 input directory: each seq → one FASTA\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def split_fasta_for_af2(fasta, out_dir):\n",
    "    if os.path.exists(out_dir):\n",
    "        shutil.rmtree(out_dir)\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "    lines = open(fasta).read().split()\n",
    "    for i in range(0, len(lines), 2):\n",
    "        hdr = lines[i]\n",
    "        seq = lines[i+1]\n",
    "        idx = int(hdr[1:])\n",
    "        with open(os.path.join(out_dir, f\"seq{idx}.fasta\"), \"w\") as f:\n",
    "            f.write(f\">seq{idx}\\n{seq}\\n\")\n",
    "\n",
    "    print(f\"[OK] AF2 input FASTA files created → {out_dir}\")\n",
    "\n",
    "split_fasta_for_af2(\"step4_missing_ProtBert_output.fasta\",'./step4_missing_af2_input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run AF2\n",
    "\n",
    "XLA_PYTHON_CLIENT_PREALLOCATE=false CUDA_VISIBLE_DEVICES=3 python3 /home/wangyu/gitlab/alphafold-dev/run_single_without_msa.py --input_type=dir --input=./step4_missing_af2_input --output_dir=./step4_missing_af2_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rank0(raw_dir, out_dir):\n",
    "    import shutil\n",
    "    if os.path.exists(out_dir):\n",
    "        shutil.rmtree(out_dir)\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "    files = os.listdir(raw_dir)\n",
    "    for f in files:\n",
    "        sub = os.path.join(raw_dir, f)\n",
    "        try:\n",
    "            shutil.copy(\n",
    "                os.path.join(sub, \"ranked_0.pdb\"),\n",
    "                os.path.join(out_dir, f\"{int(f[3:])}.pdb\")\n",
    "            )\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    print(f\"[OK] AF2 rank0 PDB extracted → {out_dir}\")\n",
    "extract_rank0(\"./step4_missing_af2_output\",\"./step4_missing_af2_output_analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate DSSP bash\n",
    "def build_dssp_sh(pdb_dir, ss_dir, sh_file):\n",
    "    if not os.path.exists(ss_dir):\n",
    "        os.makedirs(ss_dir)\n",
    "    with open(sh_file, \"w\") as f:\n",
    "        for p in os.listdir(pdb_dir):\n",
    "            src = os.path.join(pdb_dir, p)\n",
    "            tgt = os.path.join(ss_dir, p.replace(\".pdb\",\".txt\"))\n",
    "            f.write(f\"dssp {src} {tgt}\\n\")\n",
    "    print(f\"[OK] DSSP run script → {sh_file}\")\n",
    "\n",
    "\n",
    "# Parse DSSP output\n",
    "def parse_dssp_txt(dssp_txt):\n",
    "    aa, ss = \"\", \"\"\n",
    "    below = False\n",
    "    with open(dssp_txt) as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"#\"):\n",
    "                below = True\n",
    "                continue\n",
    "            if below and line.split():\n",
    "                try:\n",
    "                    if line[13].strip():\n",
    "                        aa += line[13]\n",
    "                        ss += (line[16] if line[16] != \" \" else \"-\")\n",
    "                except:\n",
    "                    continue\n",
    "    return aa, ss\n",
    "\n",
    "\n",
    "def collect_af2_dssp(ss_dir, out_csv):\n",
    "    rows = []\n",
    "    for txt in os.listdir(ss_dir):\n",
    "        if not txt.endswith(\".txt\"):\n",
    "            continue\n",
    "        aa, ss = parse_dssp_txt(os.path.join(ss_dir, txt))\n",
    "        length = len(ss)\n",
    "        non_coil = ss.count(\"H\") + ss.count(\"E\") + ss.count(\"G\") + ss.count(\"B\") + ss.count(\"I\")\n",
    "        coil_percent = (length - non_coil) / length\n",
    "        idx = int(txt.split(\".\")[0])\n",
    "        rows.append([idx, aa, ss, coil_percent])\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"id\",\"Sequence\",\"ss\",\"af2_coil_percent\"])\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(f\"[OK] AF2-DSSP collected → {out_csv}\")\n",
    "\n",
    "build_dssp_sh('step4_missing_af2_output_analysis','step4_missing_ss','step4_missing_run.sh')\n",
    "# bash step4_missing_run.sh\n",
    "collect_af2_dssp(\"step4_missing_ss\",\"step4_missing_af2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge three secondary-structure predictions and filter\n",
    "def merge_and_filter_final(\n",
    "    step3_csv, s4_csv, prot_csv, af2_csv,\n",
    "    output_csv,\n",
    "    th_s4=0.99, th_prot=0.97, th_af2=0.96\n",
    "):\n",
    "    df0 = pd.read_csv(step3_csv)\n",
    "    df0[\"id\"] = range(len(df0))\n",
    "\n",
    "    df_s4 = pd.read_csv(s4_csv)\n",
    "    df_prot = pd.read_csv(prot_csv)\n",
    "    df_af2 = pd.read_csv(af2_csv)\n",
    "\n",
    "    df = df0.merge(df_s4, on=\"id\", how=\"inner\")\n",
    "    df = df.merge(df_prot, on=\"id\", how=\"inner\")\n",
    "    df = df.merge(df_af2, on=\"id\", how=\"inner\")\n",
    "\n",
    "    df_final = df[\n",
    "        (df[\"s4_coil_percent\"] >= th_s4) &\n",
    "        (df[\"protbert_coil_percent\"] >= th_prot) &\n",
    "        (df[\"af2_coil_percent\"] >= th_af2)\n",
    "    ]\n",
    "\n",
    "    df_final.to_csv(output_csv, index=False)\n",
    "    print(f\"[OK] Step 4 final filtered motifs → {output_csv}\")\n",
    "\n",
    "step3_csv = 'step3_missing.csv'\n",
    "s4_csv = 'step4_missing_S4PRED.csv'\n",
    "prot_csv = 'step4_missing_ProtBert.csv'\n",
    "af2_csv = 'step4_missing_af2.csv'\n",
    "output_csv = 'step4_missing.csv'\n",
    "merge_and_filter_final(step3_csv, s4_csv, prot_csv, af2_csv,output_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 Amino acid modification and re-prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# --- Amino acid classes ---\n",
    "CLASS1 = set([\"R\",\"N\",\"C\",\"Q\",\"I\",\"K\",\"M\",\"F\",\"W\"])  # undesirable\n",
    "CLASS2 = [\"A\",\"G\",\"T\",\"S\",\"P\",\"E\",\"D\"]               # preferred\n",
    "\n",
    "\n",
    "def remove_class1(seq):\n",
    "    \"\"\"Remove Class1 amino acids.\"\"\"\n",
    "    return \"\".join([aa for aa in seq if aa not in CLASS1])\n",
    "\n",
    "\n",
    "def replace_class1(seq):\n",
    "    \"\"\"Replace Class1 amino acids with random Class2 amino acids.\"\"\"\n",
    "    new_seq = []\n",
    "    for aa in seq:\n",
    "        if aa in CLASS1:\n",
    "            new_seq.append(random.choice(CLASS2))\n",
    "        else:\n",
    "            new_seq.append(aa)\n",
    "    return \"\".join(new_seq)\n",
    "\n",
    "\n",
    "def step5_generate_modified_sequences(input_csv, output_csv):\n",
    "    \"\"\"\n",
    "    Step 5: For each sequence, generate:\n",
    "       - remove version: delete all Class1 AAs\n",
    "       - replace version: replace Class1 AAs with random AA from Class2\n",
    "\n",
    "    Output CSV:\n",
    "        id,orig_seq,remove_seq,replace_seq\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    rows = []\n",
    "    for i, row in df.iterrows():\n",
    "        seq = row[\"Sequence\"]\n",
    "        seq_remove = remove_class1(seq)\n",
    "        seq_replace = replace_class1(seq)\n",
    "\n",
    "        rows.append([\n",
    "            row[\"id\"] if \"id\" in row else i,\n",
    "            seq,\n",
    "            seq_remove,\n",
    "            seq_replace\n",
    "        ])\n",
    "\n",
    "    outdf = pd.DataFrame(rows, columns=[\"id\",\"orig_seq\",\"remove_seq\",\"replace_seq\"])\n",
    "    outdf.to_csv(output_csv, index=False)\n",
    "    print(f\"[OK] Step5 finished → {output_csv}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step4_missing_file = './step4_missing.csv'\n",
    "step5_missing_candidates_file = './step4_missing_candidates.csv'\n",
    "step5_generate_modified_sequences(step4_missing_file, step5_missing_candidates_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondary Filtering Based on Step 4 Results\n",
    "\n",
    "Based on the filters applied in Step 4, we perform an additional round of screening on the resulting candidate sequences. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "af",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
