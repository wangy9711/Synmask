{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling without replacement, multi-motif concatenation\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The final concatenated sequence is expected to have a total length between 400 and 800 amino acids. \n",
    "   Since each motif is approximately 20 residues long, the number of motifs to be concatenated should range from 20 to 40.\n",
    "\n",
    "2. Each motif initially has an equal probability of being selected. \n",
    "   However, as certain motifs are chosen, their probability of being selected again should gradually decrease.\n",
    "\n",
    "3. For the resulting sequences, discard any that contain fewer than 16 unique motifs to maintain overall diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def sample(candidates_id, motif_num, long_motifs: list, times=100) -> set:\n",
    "    # Repeat each candidate motif motif_num times and perform sampling without replacement,\n",
    "    # so that the probability of selection decreases as a motif is chosen more often.\n",
    "    # long_motifs is a list of IDs corresponding to long motifs\n",
    "    candidates_id = list(np.repeat(candidates_id, motif_num))\n",
    "    samples = set()\n",
    "    \n",
    "    # Perform sampling 'times' times\n",
    "    while len(samples) < times:\n",
    "        \n",
    "        # If long motifs are not considered, sampling can be done directly:\n",
    "        # sam = random.sample(candidates_id, motif_num)\n",
    "        # sam = \"-\".join([str(i) for i in sam])\n",
    "        \n",
    "        # Consider long motifs\n",
    "        tmp, tmp_count, tmp_candidate = [], motif_num, [i for i in candidates_id]\n",
    "        while tmp_count > 0:\n",
    "            id = random.choice(tmp_candidate)\n",
    "            if tmp_count >= 2:\n",
    "                if id in long_motifs:\n",
    "                    tmp_count -= 1\n",
    "            else:\n",
    "                # When only one motif remains to be selected, long motifs are not allowed\n",
    "                while id in long_motifs:\n",
    "                    id = random.choice(tmp_candidate)\n",
    "            tmp_count -= 1\n",
    "            tmp_candidate.remove(id)      \n",
    "            tmp.append(str(id))\n",
    "        samples.add(\"-\".join(tmp))\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_origin = pd.read_csv(\"/home/wangyu/projects/synmask/data/peptide/top20/peptide.csv\")\n",
    "motif_id_candidates = df_origin['ID'].to_list()\n",
    "motif_seqs_candidates = df_origin['Sequence'].to_list()\n",
    "\n",
    "motif_candidates = dict((x, y) for x, y in zip(motif_id_candidates, motif_seqs_candidates))\n",
    "\n",
    "# print(motif_candidates)\n",
    "range1, range2 = 400, 800\n",
    "long_motifs = [76]\n",
    "motif_nums = [int(i / 20) for i in range(range1, range2 + 100, 100)]\n",
    "times = [60000, 5000, 5000, 3000, 3000]\n",
    "samples = {}  # key is motif_num, value is a set of 100 concatenated sequences\n",
    "for i, motif_num in enumerate(motif_nums):\n",
    "    samples[motif_num] = sample(motif_id_candidates, motif_num, long_motifs, times[i])\n",
    "\n",
    "print(motif_nums)\n",
    "for i in samples[motif_nums[0]]:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out sequences that contain fewer than 16 unique motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of repeated motifs in a single sequence\n",
    "from collections import Counter\n",
    "\n",
    "def diversity_repeat(samples, v):\n",
    "    # Not all sequences with motif_num = 20 will contain 20 unique motifs,\n",
    "    # since long motifs may occupy multiple positions; use v as the target motif count\n",
    "    sample = samples[v]\n",
    "    # Store the diversity and repeat information for each sequence\n",
    "    screened_sample = []\n",
    "    for i in sample:\n",
    "        # Evaluate each sequence\n",
    "        ids = i.split(\"-\")\n",
    "        each_id_repeats_time = dict(sorted(Counter(ids).items(), key=lambda x: x[1], reverse=True))\n",
    "        # print(each_id_repeats_time) # This shows how many unique motifs exist in the sequence\n",
    "        if len(each_id_repeats_time.keys()) > 16:\n",
    "            seq_info = [i]\n",
    "            # Create a vector of length motif_num, where each position counts how many motifs were repeated 1x, 2x, ..., v times\n",
    "            repeat_num = [0 for _ in range(v)]\n",
    "            # Count repeat frequencies\n",
    "            # {4:3} means 3 unique motifs appeared 4 times\n",
    "            repeat_nums_times = Counter(each_id_repeats_time.values())\n",
    "            for rn, mn in repeat_nums_times.items():\n",
    "                # Record repeat count\n",
    "                repeat_num[rn - 1] = mn\n",
    "            seq_info.extend(repeat_num)\n",
    "            screened_sample.append(seq_info)\n",
    "    return screened_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_info = {}\n",
    "for v in motif_nums:\n",
    "    all_info[v] = diversity_repeat(samples, v)\n",
    "print(len(all_info[40]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save all these data to CSV files based on motif_num,apply multi-criteria sorting, and append the corresponding real amino acid sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "\n",
    "new_samples = {}\n",
    "output_path = \"/home/wangyu/projects/synmask/data/sample\"\n",
    "for v in motif_nums:\n",
    "    random_sample = all_info[v]\n",
    "    if len(all_info[v]) > 1000:\n",
    "        random_sample = random.sample(all_info[v],1000)\n",
    "    nums_col = [i for i in range(1,v+1)]\n",
    "    ascend = [False for _ in range(v)]\n",
    "    col = ['seq']+nums_col\n",
    "    df = pd.DataFrame(random_sample, columns=col)\n",
    "    # sequence\n",
    "    seqs = df['seq'].tolist()\n",
    "    aa_seqs = [\"\".join([motif_candidates[int(j)] for j in i.split(\"-\")]) for i in seqs]\n",
    "    df['aa_seq'] = aa_seqs\n",
    "    new_samples[v] = seqs\n",
    "    df.sort_values(nums_col, ascending=ascend,inplace=True)\n",
    "    # Remove columns that contain only zeros\n",
    "    df=df.loc[:, (df != 0).any(axis=0)]\n",
    "    df.to_csv(os.path.join(output_path, f\"m_{v}/sample_{v}.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "\n",
    "for v in motif_nums:\n",
    "    classification = f\"~{v*20}aa\"\n",
    "    tmp = []\n",
    "    for i in new_samples[v]:\n",
    "        tmp.extend([int(j) for j in i.split(\"-\")])\n",
    "    counter = Counter(tmp)\n",
    "    for id, count in counter.items():\n",
    "        lines.append([id, count/len(new_samples[v]), classification])\n",
    "df = pd.DataFrame(lines, columns =['id', 'count', 'type'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "# Draw a nested barplot by species and sex\n",
    "g = sns.catplot(\n",
    "    data=df, kind=\"bar\",\n",
    "    x=\"id\", y=\"count\", hue=\"type\", palette=\"dark\", alpha=.6, height=5, aspect=2\n",
    ")\n",
    "g.despine(left=True)\n",
    "g.set_axis_labels(\"\",\"Frequency of motif in one sequence\")\n",
    "g.legend.set_title(\"\")\n",
    "g.fig.suptitle('Frequency of Motif in Different Length of Sequences')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Secondary structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/home/wangyu/projects/synmask/data/peptide/top20/peptide.csv\")\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSIPred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_merged_fasta(file, fasta_file):\n",
    "    df = pd.read_csv(file)\n",
    "    seqs = df['aa_seq'].to_list()\n",
    "\n",
    "    with open(fasta_file, \"w\") as f:\n",
    "        for i, seq in tqdm(enumerate(seqs)):\n",
    "            f.writelines(f\">{i}\\n{seq}\\n\")\n",
    "\n",
    "for n in motif_num:\n",
    "    file = f\"/home/wangyu/projects/synmask/data/sample/m_{n}/sample_{n}.csv\"\n",
    "    fasta_file = f\"/home/wangyu/projects/synmask/data/sample/m_{n}/sample_{n}.fasta\"\n",
    "    get_merged_fasta(file, fasta_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run PSIPred\n",
    "\n",
    "Use the repository: https://github.com/psipred/s4pred\n",
    "\n",
    "After installation, run the following command:\n",
    "\n",
    "python /home/wangyu/gitlab/ss_predict/s4pred-update/run_model.py --device gpu --outfmt fas /home/wangyu/projects/synmask/data/sample/m_40/sample_40.fasta > /home/wangyu/projects/synmask/data/sample/m_40/s4pred.fasta\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get predicted results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def extract_s4pred(s4pred_file,output_file):\n",
    "    seqs, ss, percents = [], [], []\n",
    "    with open(s4pred_file, \"r\") as f:\n",
    "        for i, line in tqdm(enumerate(f.readlines())):\n",
    "            if i%3 == 1:\n",
    "                seqs.append(line.strip())\n",
    "            if i%3 == 2:\n",
    "                tmp = line.strip()\n",
    "                ss.append(tmp)\n",
    "                per = tmp.count(\"C\")/len(tmp)\n",
    "                percents.append(per)\n",
    "    dict = {'seq': seqs, 's4_pred_ss': ss, 's4_pred_percent': percents} \n",
    "    df = pd.DataFrame(dict)\n",
    "    df.index.name='id'\n",
    "    df.to_csv(output_file)\n",
    "\n",
    "for n in motif_num:\n",
    "    s4pred_file = f\"/home/wangyu/projects/synmask/data/sample/m_{n}/s4pred.fasta\"\n",
    "    output_file = f\"/home/wangyu/projects/synmask/data/sample/m_{n}/s4pred.csv\"\n",
    "    extract_s4pred(s4pred_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ss_filter(file, sample_file, output, col, th):\n",
    "    df = pd.read_csv(file)\n",
    "    df_coil = df[df[col]>=th]\n",
    "    df_coil=df_coil.loc[:, (df_coil != 0).any(axis=0)]   \n",
    "    seq_list = df_coil[\"id\"].to_list()\n",
    "    with open(sample_file, \"r\") as f:\n",
    "        lines = []\n",
    "        for i, line in enumerate(f.read().split()):\n",
    "            if i//2 in seq_list:\n",
    "                lines.append(line)\n",
    "    with open(output, \"w\") as f:\n",
    "        for line in lines:\n",
    "            f.writelines(line+\"\\n\")\n",
    "\n",
    "th = 0.99\n",
    "\n",
    "# Used to filter FASTA sequences for ProtBert input\n",
    "for n in motif_num:\n",
    "    s4pred_file = f\"/home/wangyu/projects/synmask/data/sample/m_{n}/s4pred.csv\"\n",
    "    sample_file = f\"/home/wangyu/projects/synmask/data/sample/m_{n}/sample_{n}.fasta\"\n",
    "    output_file = f\"/home/wangyu/projects/synmask/data/sample/m_{n}/sample_{n}_s4_1.fasta\"\n",
    "    ss_filter(s4pred_file, sample_file, output_file, 's4_pred_percent', th)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ProtBert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "def read_csv(csv_file):\n",
    "    content = pd.read_csv(csv_file)\n",
    "    print(content.columns)\n",
    "    return content \n",
    "\n",
    "def save_csv(res, csv_file):\n",
    "    with open(csv_file, \"w\") as f:\n",
    "        for row in res:\n",
    "            f.writelines(row)\n",
    "\n",
    "def read_txt(txt_file):\n",
    "    with open(txt_file,'r') as f:\n",
    "        seq = f.read().split()\n",
    "    return seq\n",
    "\n",
    "def read_fasta(fasta_file):\n",
    "    seq = []\n",
    "    with open(fasta_file,'r') as f:\n",
    "        lines = f.read().split()\n",
    "        for line in lines:\n",
    "            if not line.startswith(\">\"):\n",
    "                seq.append(line)\n",
    "    return seq\n",
    "\n",
    "def write_txt(seqs,txt_name):\n",
    "    with open(txt_name,'w') as f:\n",
    "        for i in seqs:\n",
    "            f.write(i+'\\n')\n",
    "\n",
    "def read_numpy(file_name):\n",
    "    content = np.load(file_name)\n",
    "    print(content.shape)\n",
    "    return content\n",
    "\n",
    "def ss3_model(input_list):\n",
    "    # Add a space between every amino acid in the sequence\n",
    "    seq_list = []\n",
    "    for seq in input_list:\n",
    "        seq_add_space = ''\n",
    "        for i in range(len(seq)):\n",
    "            seq_add_space += seq[i]\n",
    "            seq_add_space += ' '\n",
    "        seq_list.append(seq_add_space[:-1])\n",
    "\n",
    "    pipeline = TokenClassificationPipeline(\n",
    "        model=AutoModelForTokenClassification.from_pretrained(\"Rostlab/prot_bert_bfd_ss3\"),\n",
    "        tokenizer=AutoTokenizer.from_pretrained(\"Rostlab/prot_bert_bfd_ss3\", skip_special_tokens=True)\n",
    "        # device='cpu'\n",
    "    )\n",
    "    sequences = [re.sub(r\"[UZOB]\", \"X\", sequence) for sequence in seq_list]\n",
    "    results = pipeline(sequences)\n",
    "    return results\n",
    "\n",
    "def count_ss3_each_seq(ss3_model_results):\n",
    "    res = []\n",
    "    for result in ss3_model_results:\n",
    "        line = ''\n",
    "        str_result = ''\n",
    "        for amino in result:\n",
    "            str_result += amino['entity']\n",
    "        counter = Counter(str_result)\n",
    "        res.append(str_result + \",\" + str(counter[\"C\"]/len(str_result))+\"\\n\")\n",
    "    return res\n",
    "\n",
    "def count_R_K_num(seqs):\n",
    "    nums = []\n",
    "    for s in seqs:\n",
    "        nums.append(s.count('R') + s.count('K'))\n",
    "    return nums\n",
    "\n",
    "def seq_sencond_struct(seqs, csv_name, R_K_num=False):\n",
    "    results = ss3_model(seqs)\n",
    "    c_list, h_list, e_list = count_ss3_each_seq(results)\n",
    "    content = {}\n",
    "    content['sequence'] = seqs\n",
    "    content['coil(%)'] = c_list\n",
    "    content['helix(%'] = h_list\n",
    "    content['strand(%)'] = e_list\n",
    "    if R_K_num:\n",
    "        R_K_list = count_R_K_num(seqs)\n",
    "        content['R+K'] = R_K_list\n",
    "    content = pd.DataFrame(content)\n",
    "    content.to_csv(csv_name, index=False)\n",
    "\n",
    "# Predict secondary structure for each sequence\n",
    "seqs = read_fasta('/home/wangyu/projects/synmask/data/sample/m_20/sample_20_s4_1.fasta')\n",
    "results = ss3_model(seqs)\n",
    "res = count_ss3_each_seq(results)\n",
    "save_csv(res, '/home/wangyu/projects/synmask/data/sample/m_20/prot_bert.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "th = 0.97\n",
    "\n",
    "for n in motif_num:\n",
    "    prot_file = f\"/home/wangyu/projects/synmask/data/sample/m_{n}/prot_bert.csv\"\n",
    "    sample_file = f\"/home/wangyu/projects/synmask/data/sample/m_{n}/sample_{n}_s4_1.fasta\"\n",
    "    \n",
    "    with open(sample_file, \"r\") as f:\n",
    "        seq_list = []\n",
    "        for i, line in enumerate(f.read().split()):\n",
    "            if i%2 ==0:\n",
    "                seq_list.append(int(line[1:]))\n",
    "                \n",
    "    df = pd.read_csv(prot_file)\n",
    "    df['id'] = seq_list\n",
    "    df.to_csv(prot_file, index=False)\n",
    "    output_file = f\"/home/wangyu/projects/synmask/data/sample/m_{n}/sample_{n}_prot_2.fasta\"\n",
    "    ss_filter(prot_file, sample_file, output_file, 'prot_percent', th)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alphafold2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split fasta files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "def fasta2_split_fasta(file, fasta_folder):\n",
    "    with open(file, \"r\") as f:\n",
    "        lines = f.read().split()\n",
    "        for i in range(len(lines)):\n",
    "            if lines[i].startswith(\">\"):\n",
    "                with open(os.path.join(fasta_folder,f\"seq{lines[i][1:]}.fasta\"), \"w\") as f:\n",
    "                    f.writelines(f\">seq{i}\\n{lines[i+1]}\\n\")\n",
    "            \n",
    "for n in motif_num:\n",
    "    file = f\"/home/wangyu/projects/synmask/data/sample/m_{n}/sample_{n}_prot_2.fasta\"\n",
    "    fasta_folder = f\"/home/wangyu/projects/synmask/data/sample/m_{n}/af/input\"\n",
    "    output = f\"/home/wangyu/projects/synmask/data/sample/m_{n}/af/output\"\n",
    "    if os.path.exists(fasta_folder):\n",
    "        shutil.rmtree(fasta_folder)\n",
    "    if os.path.exists(output):\n",
    "        shutil.rmtree(output)\n",
    "    os.makedirs(fasta_folder)\n",
    "    os.makedirs(output)\n",
    "    fasta2_split_fasta(file, fasta_folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run AF2\n",
    "\n",
    "XLA_PYTHON_CLIENT_PREALLOCATE=false CUDA_VISIBLE_DEVICES=3 python3 /home/wangyu/gitlab/alphafold-dev/run_single_without_msa.py --input_type=dir --input=/home/wangyu/projects/synmask/data/sample/m_40/af/input --output_dir=/home/wangyu/projects/synmask/data/sample/m_40/af/output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the predicted results from AF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "def extract_rank0(raw_output_folder, rank0_folder):\n",
    "    raw_files = os.listdir(raw_output_folder)\n",
    "    \n",
    "    if os.path.exists(rank0_folder):\n",
    "        shutil.rmtree(rank0_folder)    \n",
    "    os.makedirs(rank0_folder)\n",
    "    \n",
    "    for file in tqdm(raw_files):\n",
    "        id = int(file[3:])\n",
    "        try:\n",
    "            rank0_pdb = os.path.join(os.path.join(raw_output_folder,file),\"ranked_0.pdb\")\n",
    "            shutil.copy(rank0_pdb, os.path.join(rank0_folder,f\"{id}.pdb\"))\n",
    "        except Exception:\n",
    "            print(n, id)\n",
    "            \n",
    "        \n",
    "for n in motif_num:\n",
    "    raw_output_folder = f\"/home/wangyu/projects/synmask/data/sample/m_{n}/af/output\"\n",
    "    rank0_folder = f\"/home/wangyu/projects/synmask/data/sample/m_{n}/af/output_rank0\"\n",
    "    extract_rank0(raw_output_folder, rank0_folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last line contains two lines of code that calculate and analyze the AF results using DSSP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "import pathlib\n",
    "import csv\n",
    "import shutil\n",
    "\n",
    "def generate_ssfile(pdb_path,ssfile_path,output_sh):\n",
    "    print(\"generating ssfile sh...\")\n",
    "    f = open(output_sh, \"a\")\n",
    "    files = os.listdir(pdb_path)\n",
    "    for file in files:\n",
    "        f.write(f\"dssp {os.path.join(pdb_path,file)} {os.path.join(ssfile_path,pathlib.Path(file).stem+'.txt')}\\n\")\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def list2txt(output, file):\n",
    "    with open(file, 'w') as f:\n",
    "        w = csv.writer(f)    \n",
    "        for line in output:\n",
    "            w.writerow(line)    \n",
    "    print(\"Saved to file.\")\n",
    "\n",
    "\n",
    "def convert_ssfile(ssfile_txt):\n",
    "    dict = defaultdict(str)\n",
    "    below_header = False\n",
    "    with open(ssfile_txt, \"r\") as f:\n",
    "        for line in f:\n",
    "            if line.strip().startswith(\"#\"):\n",
    "                below_header = True\n",
    "                continue\n",
    "            if below_header and line.split():\n",
    "                if line.strip().split()[1].isnumeric():\n",
    "                    dict[\"ss\"] += line[16] if line[16] != \" \" else \"-\"\n",
    "                    dict[\"aa\"] += line[13]\n",
    "    return dict\n",
    "\n",
    "\n",
    "def all_ssfile2txt(ssfile_path, output_file):\n",
    "    print(\"starting convert ssfile to one file...\")\n",
    "    files = os.listdir(ssfile_path)\n",
    "    output = []\n",
    "    for file in tqdm(files):\n",
    "        dict = convert_ssfile(os.path.join(ssfile_path,file))\n",
    "        sturcture = ['H','G','I','E','B']\n",
    "        ss_count = 0\n",
    "        for s in sturcture:\n",
    "            ss_count += dict[\"ss\"].count(s)\n",
    "        percent = (len(dict[\"ss\"])-ss_count)/len(dict[\"ss\"])\n",
    "        output.append([pathlib.Path(file).stem,dict[\"aa\"],dict[\"ss\"], percent])\n",
    "\n",
    "    list2txt(output,output_file)\n",
    "    \n",
    "    \n",
    "for n in motif_num:\n",
    "    # 生成dssp的sh file\n",
    "    pdb_path = f\"/home/wangyu/projects/synmask/data/sample/m_{n}/af/output_rank0\"\n",
    "    ssfile_path = f\"/home/wangyu/projects/synmask/data/sample/m_{n}/af/af_ss\"\n",
    "    if not os.path.exists(ssfile_path):\n",
    "        os.makedirs(ssfile_path)\n",
    "    output_sh = f\"/home/wangyu/projects/synmask/data/sample/m_{n}/af/ssfile.sh\"\n",
    "    output_file = f\"/home/wangyu/projects/synmask/data/sample/m_{n}/af/af_ss.txt\"\n",
    "    # Generate bash file\n",
    "    generate_ssfile(pdb_path,ssfile_path,output_sh)\n",
    "    # Convert to .txt\n",
    "    # all_ssfile2txt(ssfile_path, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filters = [1,1,0.99,0.96,0.94]\n",
    "for i, n in enumerate(motif_num):\n",
    "    af_file = f\"/home/wangyu/projects/synmask/data/sample/m_{n}/af/af_ss.csv\"\n",
    "    sample_file = f\"/home/wangyu/projects/synmask/data/sample/m_{n}/sample_{n}_prot_2.fasta\"\n",
    "    output_file = f\"/home/wangyu/projects/synmask/data/sample/m_{n}/sample_{n}_af_3.fasta\"\n",
    "    ss_filter(af_file, sample_file, output_file, 'af_percent', filters[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id(fasta_file):\n",
    "    with open(fasta_file, \"r\") as f:\n",
    "        seq_list = []\n",
    "        for i, line in enumerate(f.read().split()):\n",
    "            if i%2 ==0:\n",
    "                seq_list.append(int(line[1:]))\n",
    "    return seq_list\n",
    "\n",
    "for n in motif_num:\n",
    "    fasta_file = f\"/home/wangyu/projects/synmask/data/sample/m_{n}/sample_{n}_af_3.fasta\"\n",
    "    seq_list = get_id(fasta_file)\n",
    "    df = pd.DataFrame()\n",
    "    df['id'] = seq_list\n",
    "    \n",
    "    origin_file = f\"/home/wangyu/projects/synmask/data/sample/m_{n}/sample_{n}.csv\"\n",
    "    af_file = f\"/home/wangyu/projects/synmask/data/sample/m_{n}/af/af_ss.csv\"\n",
    "    s4pred_file = f\"/home/wangyu/projects/synmask/data/sample/m_{n}/s4pred.csv\"\n",
    "    prot_file = f\"/home/wangyu/projects/synmask/data/sample/m_{n}/prot_bert.csv\"\n",
    "    out = f\"/home/wangyu/projects/synmask/data/sample/m_{n}/merged_ss.csv\"\n",
    "    \n",
    "    df_af = pd.read_csv(af_file)\n",
    "    df_s4 = pd.read_csv(s4pred_file)\n",
    "    df_prot = pd.read_csv(prot_file)\n",
    "    df_origin = pd.read_csv(origin_file)\n",
    "    df_origin.index.name = \"id\"\n",
    "    \n",
    "    df = df.merge(df_origin,how='inner',on='id')\n",
    "    df = df.merge(df_af,how='inner',on='id')\n",
    "    df = df.merge(df_s4,how='inner',on='id')\n",
    "    df = df.merge(df_prot,how='inner',on='id')\n",
    "    ds = [\"seq_y\",\"af_ss\",\"seq\",\"s4_pred_ss\",\"prot_ss\"]\n",
    "    for d in ds:\n",
    "        del df[d]\n",
    "    df.to_csv(out,index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the prediction results of secondary structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "for n in motif_num:\n",
    "\n",
    "    df_prot = pd.read_csv(f\"/home/wangyu/projects/synmask/data/sample/m_{n}/prot_bert.csv\",header=None)\n",
    "    df_prot.columns=['ss','percent']\n",
    "    \n",
    "    df_s4 = pd.read_csv(f\"/home/wangyu/projects/synmask/data/sample/m_{n}/s4pred.csv\")\n",
    "    df_s4.columns=['id','seq','ss','percent']\n",
    "    \n",
    "    df_af = pd.read_csv(f\"/home/wangyu/projects/synmask/data/sample/m_{n}/af/af_ss.txt\",header=None)\n",
    "    df_af.columns = ['id','seq','ss','percent']\n",
    "    df_af = df_af.sort_values(\"id\")\n",
    "    \n",
    "    seqs = df_s4['seq'].to_list()\n",
    "    per_prot = df_prot['percent'].to_list()\n",
    "    per_s4 = df_s4['percent'].to_list()\n",
    "    per_af = df_af['percent'].to_list()\n",
    "    dict_percent = {'seq': seqs, 'af': per_af, 'prot_bert': per_prot, 'psi_pred': per_s4} \n",
    "    df = pd.DataFrame(dict_percent)\n",
    "    df.to_csv(f\"/home/wangyu/projects/synmask/data/sample/m_{n}/merged_ss.csv\",index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Immunogenicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_allele(allele_file):\n",
    "    with open(allele_file, \"r\") as f:\n",
    "        outputs = [line.strip() for line in f.readlines()]\n",
    "    return outputs\n",
    "\n",
    "\n",
    "allele_i_file= f\"/home/wangyu/projects/synmask/code/immuno/allele_i\"\n",
    "allele_ii_file= f\"/home/wangyu/projects/synmask/code/immuno/allele_ii\"\n",
    "\n",
    "allele_i = get_common_allele(allele_i_file)\n",
    "allele_ii = get_common_allele(allele_ii_file)\n",
    "print(len(allele_i),len(allele_ii))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a script to run immunogenicity prediction.\n",
    "\n",
    "Use the predictor provided by \n",
    "https://services.healthtech.dtu.dk/services/NetMHCpan-4.1/ and https://services.healthtech.dtu.dk/services/NetMHCIIpan-4.1/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def bash(mhc_type, input_fasta, allele, output_data_folder, output_bash_file, len):\n",
    "    flag = \"\" if mhc_type else \" -l 10\"\n",
    "    with open(output_bash_file, \"w\") as f:\n",
    "        f.writelines(f\"tmux new -d -s mhc_{mhc_type}_{len}\\n\")\n",
    "        for i,a in enumerate(allele):\n",
    "            f.writelines(f\"tmux new-window -n window{i} -t  mhc_{mhc_type}_{len}\\n\")\n",
    "            f.writelines(f\"tmux send -t  mhc_{mhc_type}_{len}:window{i} \\\"/home/wangyu/gitlab/ImmunoPredictTool/NetMHCpan/netMHC{mhc_type}pan-4.1/netMHC{mhc_type}pan -f {input_fasta} -xls -a {a}{flag} -xlsfile {os.path.join(output_data_folder,a+'.csv')}\\n\\\"ENTER\\n\")\n",
    "\n",
    "\n",
    "\n",
    "for n in motif_num:\n",
    "    fasta_file = f\"/home/wangyu/projects/synmask/data/sample/m_{n}/sample_{n}_af_3.fasta\"\n",
    "    output_i = f\"/home/wangyu/projects/synmask/data/sample/m_{n}/immuno/i/predict_data\"\n",
    "    output_ii = f\"/home/wangyu/projects/synmask/data/sample/m_{n}/immuno/ii/predict_data\"\n",
    "    \n",
    "    if os.path.exists(output_i):\n",
    "        shutil.rmtree(output_i)\n",
    "    if os.path.exists(output_ii):\n",
    "        shutil.rmtree(output_ii)\n",
    "    os.makedirs(output_i)\n",
    "    os.makedirs(output_ii)\n",
    "    output_bash_i = f\"/home/wangyu/projects/synmask/data/sample/m_{n}/immuno/i/bash.sh\"\n",
    "    output_bash_ii = f\"/home/wangyu/projects/synmask/data/sample/m_{n}/immuno/ii/bash.sh\"\n",
    "    \n",
    "    bash(\"\",fasta_file,allele_i,output_i,output_bash_i, n)\n",
    "    bash(\"II\",fasta_file,allele_ii,output_ii,output_bash_ii, n)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical analysis of immunogenicity prediction results and merging of secondary structure prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "\n",
    "def convert_predict_data(predict_folder, merged_csv, types=\"i\"):\n",
    "    dict = {}\n",
    "    files = os.listdir(predict_folder)\n",
    "    for file in files:\n",
    "        name = pathlib.Path(file).stem\n",
    "        df = pd.read_csv(os.path.join(predict_folder, file),header=1,sep=\"\\t\")\n",
    "        df = df.groupby(['ID'])['NB'].sum().reset_index(name=f\"sum_{types}\")\n",
    "        sums = df[f\"sum_{types}\"].to_list()\n",
    "        dict[name] = sums\n",
    "        \n",
    "    merged_df = pd.DataFrame(dict)\n",
    "    merged_df[f\"sum_{types}\"] = merged_df[list(merged_df.columns)].sum(axis=1)\n",
    "    merged_df.to_csv(merged_csv)\n",
    "    return merged_df\n",
    "\n",
    "for n in tqdm(motif_num):\n",
    "    predict_i_folder = f\"/home/wangyu/projects/synmask/data/sample/m_{n}/immuno/i/predict_data\"\n",
    "    predict_ii_folder = f\"/home/wangyu/projects/synmask/data/sample/m_{n}/immuno/ii/predict_data\"\n",
    "    \n",
    "    merged_i = f\"/home/wangyu/projects/synmask/data/sample/m_{n}/immuno/merged_i.csv\"\n",
    "    merged_ii = f\"/home/wangyu/projects/synmask/data/sample/m_{n}/immuno/merged_ii.csv\"\n",
    "    \n",
    "    all_merged = f\"/home/wangyu/projects/synmask/data/sample/m_{n}/merged.csv\"\n",
    "    # all_merged = f\"/home/wangyu/projects/synmask/data/sample/m_{n}/merged_immuno.csv\"\n",
    "    \n",
    "    merged_i_df = convert_predict_data(predict_i_folder, merged_i)\n",
    "    merged_ii_df = convert_predict_data(predict_ii_folder, merged_ii,\"ii\")\n",
    "    origin_df = pd.read_csv(f\"/home/wangyu/projects/synmask/data/sample/m_{n}/merged_ss.csv\")\n",
    "    origin_df_columns = list(origin_df.columns)\n",
    "    result = pd.concat([origin_df,merged_i_df,merged_ii_df],axis=1)[origin_df_columns+[\"sum_i\"]+[\"sum_ii\"]]\n",
    "    result['avg'] = (result[\"sum_i\"] + result[\"sum_ii\"])/result['aa_seq'].str.len()\n",
    "    result.to_csv(all_merged, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in motif_num:\n",
    "    all_merged = f\"/home/wangyu/projects/synmask/data/sample/m_{n}/merged.csv\"\n",
    "    df = pd.read_csv(all_merged)\n",
    "    # For motif 20，20/20=1，delete sequence have motif repeat more than 3 times\n",
    "    nums_col = [str(i) for i in range(1,len(list(df.columns))-8)]\n",
    "    for i in nums_col:\n",
    "        if int(i) > n/20*3:\n",
    "            df = df.drop(df[(df[i]>0)].index)\n",
    "    df=df.loc[:, (df != 0).any(axis=0)]   # 删除全是0的列\n",
    "    df = df.sort_values(['avg'], ascending=True)\n",
    "    df.to_csv(all_merged,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def not_show(current_df):\n",
    "    splic_ids = current_df['seq_x']\n",
    "    df_origin = pd.read_csv(\"/home/wangyu/projects/synmask/data/peptide/top20/peptide.csv\")\n",
    "    origin_ids = set(df_origin['ID'].to_list())\n",
    "    ids = []\n",
    "    for i in splic_ids:\n",
    "        ids.extend([int(j) for j in i.split(\"-\")])\n",
    "    counter = Counter(ids)\n",
    "\n",
    "    show = set(counter.keys())\n",
    "    not_show = origin_ids-show\n",
    "    for i in not_show:\n",
    "        counter[i] = 0\n",
    "    sort = dict(sorted(counter.items()))\n",
    "    return sort\n",
    "\n",
    "\n",
    "\n",
    "merged_sort_df = pd.DataFrame(columns=['id','count','motif_num'])\n",
    "for n in motif_num:\n",
    "    output = f\"/home/wangyu/projects/synmask/data/sample/m_{n}/merged.csv\"\n",
    "    df = pd.read_csv(output)\n",
    "    df=df.iloc[:20,:]\n",
    "    sort = not_show(df)    \n",
    "    lst = [str(i) for i in sort.keys()]\n",
    "    lst2 = [i for i in sort.values()]\n",
    "    sort_df = pd.DataFrame(zip(lst,lst2,[n for _ in range(len(lst))]),columns=['id','count','motif_num'])\n",
    "    merged_sort_df = merged_sort_df.append(sort_df, ignore_index=True)\n",
    "print(merged_sort_df)\n",
    "    \n",
    "# fig, ax =plt.subplots(2,2,constrained_layout=True, figsize=(8, 8))\n",
    "    \n",
    "# axesSub = sns.barplot(x='id',y='count', data=sort_df, ax=ax[0])\n",
    "# axesSub.set_title(f'motif*{n}')\n",
    "\n",
    "sea = sns.FacetGrid(merged_sort_df, col_wrap=5, aspect=1.5,col = \"motif_num\")\n",
    "sea.map(sns.barplot, \"id\", \"count\", color='c')\n",
    "# for axes in sea.axes.flat:\n",
    "#     _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=30)\n",
    "# plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f4c183d4c65fbc5803d12d0972175d8438015f9e2218e5d6eceaabd8685f5444"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
